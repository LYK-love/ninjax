{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ninjax: General Modules for JAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ninjax is a general module system for JAX. It gives the user complete and transparent control over updating the state of each module, bringing the flexibility of PyTorch and TensorFlow to JAX. Moreover, Ninjax makes it easy to mix and match modules from different libraries, such as Flax and Haiku."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Motivation\n",
    "\n",
    "Existing deep learning libraries for JAX provide modules, but those modules only specify neural networks and cannot easily implement training logic. Orchestrating training all in one place, outside of the modules, is fine for simple code bases. But it becomes a problem when there are many modules with their own training logic and optimizers.\n",
    "\n",
    "Ninjax solves this problem by giving each nj.Module full read and write access to its state. This means modules can have train functions to implement custom training logic, and call each other's train functions. Ninjax is intended to be used with one or more neural network libraries, such as Haiku and Flax.\n",
    "\n",
    "The main differences to existing deep learning libraries are:\n",
    "\n",
    "* Ninjax does not need separate apply()/init() functions. Instead, the first function call creates variables automatically.\n",
    "* Ninjax lets you access and update model parameters inside of impure functions, so modules can handle their own optimizers and update logic.\n",
    "* Natural support for modules with multiple functions without need for Flax's setup() function or Haiku's hk.multi_transform().\n",
    "* Ninjax' flexible state handling makes it trivial to mix and match modules from other deep learning libraries in your models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installation\n",
    "\n",
    "Ninjax is a single file, so you can just copy it to your project directory. Or you can install the package:\n",
    "\n",
    "```sh\n",
    "pip install ninjax\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import haiku as hk\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import ninjax as nj\n",
    "import optax\n",
    "import flax.linen as nn\n",
    "\n",
    "# Ninjax supports all Haiku and Flax modules and new libraries are easy to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 0.7322518825531006\n",
      "Loss: 0.597238302230835\n",
      "Loss: 0.48707011342048645\n",
      "Loss: 0.39765995740890503\n",
      "Loss: 0.3283187747001648\n",
      "Loss: 0.27209892868995667\n",
      "Loss: 0.2248704433441162\n",
      "Loss: 0.18528717756271362\n",
      "Loss: 0.15232132375240326\n",
      "Loss: 0.12485767900943756\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nj.Module):\n",
    "\n",
    "  def __init__(self, size, lr=0.01, act=jax.nn.relu):\n",
    "    self.size = size\n",
    "    self.lr = lr\n",
    "    self.act = act\n",
    "    # Define submodules upfront.\n",
    "    self.h1 = Linear(128, name='h1')\n",
    "    self.h2 = Linear(128, name='h2')\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = self.act(self.h1(x))\n",
    "    x = self.act(self.h2(x))\n",
    "    # Define submodules inline.\n",
    "    x = self.get('h3', Linear, self.size, with_bias=False)(x)\n",
    "    # Create state entries of array values.\n",
    "    x += self.get('bias', jnp.array, 0.0)\n",
    "    return x\n",
    "\n",
    "  def train(self, x, y):\n",
    "    # Compute gradient with respect to all parameters in this module.\n",
    "    loss, params, grad = nj.grad(self.loss, self)(x, y)\n",
    "    # Update the parameters with gradient descent.\n",
    "    state = jax.tree_util.tree_map(lambda p, g: p - self.lr * g, params, grad)\n",
    "    # Update multiple state entries of this module.\n",
    "    self.putm(state)\n",
    "    return loss\n",
    "\n",
    "  def loss(self, x, y):\n",
    "    return ((self(x) - y) ** 2).mean()\n",
    "\n",
    "\n",
    "# The complete state is stored in a flat dictionary. Ninjax automatically\n",
    "# applies scopes to the string keys based on the module names.\n",
    "state = {}\n",
    "model = MyModel(8, name='MyModel')\n",
    "train = nj.pure(model.train)  # nj.jit(...), nj.pmap(...)\n",
    "main = jax.random.PRNGKey(0)\n",
    "\n",
    "# Let's train on some example data.\n",
    "dataset = [(jnp.ones((64, 32)), jnp.ones((64, 8)))] * 10\n",
    "for x, y in dataset:\n",
    "  rng, main = jax.random.split(main)\n",
    "  # Variables are automatically initialized on the first call. This adds them\n",
    "  # to the state dictionary.\n",
    "  loss, state = train(state, rng, x, y)\n",
    "  # To look at parameters, simply use the state dictionary.\n",
    "  assert state['MyModel/bias'].shape == ()\n",
    "  print('Loss:', float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I create state entries?\n",
    "Ninjax gives modules full control over reading and updating their state entries. Use `self.get(name, ctor, *args, **kwargs)` to define state entries. The first call creates the entry as `ctor(*args, **kwargs)`. Later calls return the current value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n",
      "{'ModuleToCreateStateEntries/weights': Array([[-0.04147053, -0.1314227 , -0.02465397, ...,  0.02183795,\n",
      "         0.01697004,  0.04713267],\n",
      "       [ 0.19299734, -0.11297554, -0.07765299, ...,  0.09174806,\n",
      "        -0.14098048,  0.21593362],\n",
      "       [ 0.14658523, -0.24184895, -0.14916015, ...,  0.10904205,\n",
      "         0.0512017 , -0.05426091],\n",
      "       ...,\n",
      "       [ 0.17859662,  0.0211283 , -0.00192875, ...,  0.16585249,\n",
      "         0.22263348,  0.0516718 ],\n",
      "       [ 0.10639775, -0.20550174,  0.15000093, ..., -0.00155634,\n",
      "        -0.2283287 , -0.19280887],\n",
      "       [ 0.04920799, -0.03702986,  0.17916483, ..., -0.21073604,\n",
      "         0.05745828,  0.0972172 ]], dtype=float32), 'ModuleToCreateStateEntries/bias': Array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],      dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "class ModuleToCreateStateEntries(nj.Module):\n",
    "\n",
    "  def compute(self, x):\n",
    "    init = jax.nn.initializers.variance_scaling(1, 'fan_avg', 'uniform')\n",
    "    weights = self.get('weights', init, nj.rng(), (64, 32))\n",
    "    bias = self.get('bias', jnp.zeros, (32,), jnp.float32)\n",
    "    print(self.getm())  # {'/path/to/module/weights': ..., '/path/to/module/bias': ...}\n",
    "    return x @ weights + bias\n",
    "  \n",
    "\n",
    "state = {}\n",
    "model = ModuleToCreateStateEntries(name='ModuleToCreateStateEntries')\n",
    "compute = nj.pure(model.compute)  # nj.jit(...), nj.pmap(...)\n",
    "main = jax.random.PRNGKey(0)\n",
    "\n",
    "# Let's train on some example data.\n",
    "dataset = [(jnp.ones((64, 64)), jnp.ones((64, 64)))] * 10\n",
    "for x, y in dataset:\n",
    "  rng, main = jax.random.split(main)\n",
    "  # Variables are automatically initialized on the first call. This adds them\n",
    "  # to the state dictionary.\n",
    "  loss, state = compute(state, rng, x)\n",
    "  # To look at parameters, simply use the state dictionary.\n",
    "  # print('Loss:', float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I update state entries?\n",
    "To update the state entries of a module, use `self.put(name, value)` for individual entries of `self.putm(mapping)` to update multiple values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "{'ModuleToUpdateState/counter': Array(1, dtype=int32)}\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "class ModuleToUpdateState(nj.Module):\n",
    "\n",
    "  def counting(self):\n",
    "    counter = self.get('counter', jnp.zeros, (), jnp.int32)\n",
    "    self.put('counter', counter + 1)\n",
    "    print(self.get('counter'))  # 1\n",
    "    state = self.getm() # `state` is a dictionary of all state entries, note that the key of every entry is the **full path** of the entry, not just the entry name.\n",
    "    print(state)\n",
    "    print(state['ModuleToUpdateState/counter']) # 1. It's not `state['counter']`.\n",
    "    counter = self.get('counter')\n",
    "    self.put('counter', counter + 1)\n",
    "    print(self.get('counter'))  # 2\n",
    "    \n",
    "    \n",
    "state = {}\n",
    "model = ModuleToUpdateState(name=\"ModuleToUpdateState\")\n",
    "counting = nj.pure(model.counting)  # nj.jit(...), nj.pmap(...)\n",
    "main = jax.random.PRNGKey(0)\n",
    "\n",
    "state, state = counting(state, main)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I use JIT compilation?\n",
    "The `nj.pure()` function makes the state your JAX code uses explicit, so it can be transformed freely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "{'ModuleToUpdateState/counter': Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "{'ModuleToUpdateState/counter': Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n",
      "Traced<ShapedArray(int32[])>with<DynamicJaxprTrace(level=1/0)>\n"
     ]
    }
   ],
   "source": [
    "state = {}\n",
    "counting = nj.jit(nj.pure(model.counting))  # nj.jit(...), nj.pmap(...)\n",
    "main = jax.random.PRNGKey(0)\n",
    "\n",
    "state, state = counting(state, main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value: [1 2 3]\n"
     ]
    }
   ],
   "source": [
    "from jax import jit\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def print_and_return(x):\n",
    "    print(\"Value:\", x)\n",
    "    return x\n",
    "\n",
    "@jit\n",
    "def my_function(x):\n",
    "    result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n",
    "    x = jax.pure_callback(print_and_return, result_shape, x)\n",
    "    # Continue computation with x...\n",
    "    return x ** 2\n",
    "\n",
    "# Now calling my_function will print the value of x before squaring\n",
    "result = my_function(jnp.array([1, 2, 3]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I compute gradients?\n",
    "\n",
    "You can use `jax.grad` as normal for computing gradients with respect to explicit inputs of your function. To compute gradients with respect to Ninjax state, use `nj.grad(fn, keys)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleToComputeGradients(nj.Module):\n",
    "\n",
    "  def train(self, x, y):\n",
    "    params = self.getm('.*')\n",
    "    loss, grads = nj.grad(self.loss, params.keys())(x, y)\n",
    "    params = jax.tree_map(lambda p, g: p - 0.01 * g, params, grads)\n",
    "    self.putm(params)\n",
    "    \n",
    "state = {}\n",
    "model = ModuleToComputeGradients(8, name='ModuleToComputeGradients')\n",
    "train = nj.pure(model.train)  # nj.jit(...), nj.pmap(...)\n",
    "main = jax.random.PRNGKey(0)\n",
    "\n",
    "# Let's train on some example data.\n",
    "dataset = [(jnp.ones((64, 32)), jnp.ones((64, 8)))] * 10\n",
    "for x, y in dataset:\n",
    "  rng, main = jax.random.split(main)\n",
    "  # Variables are automatically initialized on the first call. This adds them\n",
    "  # to the state dictionary.\n",
    "  loss, state = train(state, rng, x, y)\n",
    "  # To look at parameters, simply use the state dictionary.\n",
    "  assert state['MyModel/bias'].shape == ()\n",
    "  print('Loss:', float(loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `self.getm(filter='.*')` method optionally accepts a regex pattern to select only a subset of the state dictionary. It also returns only state entries of the current module. To access the global state, use `nj.state()`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I define modules compactly?\n",
    "You can use `self.get(name, ctor, *args, **kwargs)` inside methods of your modules. When called for the first time, it creates a new state entry from the constructor `ctor(*args, **kwargs)`. Later calls return the existing entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleMLP(nj.Module):\n",
    "\n",
    "  def __call__(self, x):\n",
    "    x = jax.nn.relu(self.get('h1', Linear, 128)(x))\n",
    "    x = jax.nn.relu(self.get('h2', Linear, 128)(x))\n",
    "    x = self.get('h3', Linear, 32)(x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I use Haiku modules?\n",
    "\n",
    "There is nothing special about using external libraries with Ninjax. Haiku requires its modules to be passed through `hk.transform` and the initialized via `transformed.init(rng, batch)`. For convenience, Ninjax provides `nj.HaikuModule` to do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleHaiku(nj.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.mlp = nj.HaikuModule(hk.nets.MLP, [128, 128, 32])\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.mlp(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also predefine a list of aliases for Haiku modules that you want to use frequently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Linear = functools.partial(nj.HaikuModule, hk.Linear)\n",
    "Conv2D = functools.partial(nj.HaikuModule, hk.Conv2D)\n",
    "MLP = functools.partial(nj.HaikuModule, hk.nets.MLP)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I use Flax modules?\n",
    "There is nothing special about using external libraries with Ninjax. Flax requires its modules to be initialized via `params = model.init(rng, batch)` and used via `model.apply(params, data)`. For convenience, Ninjax provides nj.FlaxModule to do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleFlax(nj.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.linear = nj.FlaxModule(nn.Dense, 128)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.linear(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also predefine a list of aliases for Flax modules that you want to use frequently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dense = functools.partial(nj.FlaxModule, nn.Dense)\n",
    "Conv = functools.partial(nj.FlaxModule, nn.Conv)\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How can I use Optax optimizers?\n",
    "There is nothing special about using external libraries like Optax with Ninjax. Optax requires its optimizers to be initialized, their state to be passed through the optimizer call, and the resulting updates to be applied. For convenience, Ninjax provides `nj.OptaxModule` to do this for you:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModuleOptax(nj.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.mlp = MLP()\n",
    "    self.opt = nj.OptaxModule(optax.adam, 1e-3)\n",
    "\n",
    "  def train(self, x, y):\n",
    "    self.mlp(x)  # Ensure paramters are created.\n",
    "    metrics = self.opt(self.mlp.getm('.*'), self.loss, x, y)\n",
    "    return metrics  # {'loss': ..., 'grad_norm': ...}\n",
    "\n",
    "  def loss(self, x, y):\n",
    "    return ((self.mlp(x) - y) ** 2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(nj.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    self.linear = nj.FlaxModule(nn.Dense, 128)\n",
    "\n",
    "  def __call__(self, x):\n",
    "    return self.linear(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Load your dataset\u001b[39;00m\n\u001b[1;32m     10\u001b[0m image_folder \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/lyk/Projects/Machine-Learning-Basic/VAE/images\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 11\u001b[0m custom_images \u001b[38;5;241m=\u001b[39m \u001b[43mload_images_from_folder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_folder\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 3\u001b[0m, in \u001b[0;36mload_images_from_folder\u001b[0;34m(folder)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_images_from_folder\u001b[39m(folder):\n\u001b[1;32m      2\u001b[0m     images \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m filename \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(folder):\n\u001b[1;32m      4\u001b[0m         img \u001b[38;5;241m=\u001b[39m imageio\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder, filename))\n\u001b[1;32m      5\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m img \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "def load_images_from_folder(folder):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder):\n",
    "        img = imageio.imread(os.path.join(folder, filename))\n",
    "        if img is not None:\n",
    "            images.append(img)\n",
    "    return np.array(images)\n",
    "\n",
    "# Load your dataset\n",
    "image_folder = '/home/lyk/Projects/Machine-Learning-Basic/VAE/images'\n",
    "custom_images = load_images_from_folder(image_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data into NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BouncingBall\n",
    "total_images = custom_images.shape[0]\n",
    "\n",
    "# For example, let's say you want a 70-15-15 split\n",
    "train_end = int(total_images * 0.7)\n",
    "valid_end = train_end + int(total_images * 0.15)\n",
    "\n",
    "train_images = custom_images[:train_end]\n",
    "valid_images = custom_images[train_end:valid_end]\n",
    "test_images = custom_images[valid_end:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dict = {'image': train_images}\n",
    "valid_data_dict = {'image': valid_images}\n",
    "test_data_dict = {'image': test_images}\n",
    "train_data_variance = np.var(train_data_dict['image'] / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_and_normalise_images(features):\n",
    "    # Assuming your images are uint8 [0, 255]\n",
    "    features['image'] = tf.cast(features['image'], tf.float32) / 255.0\n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = tfds.as_numpy(\n",
    "    tf.data.Dataset.from_tensor_slices(train_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .shuffle(10000)\n",
    "    .repeat(-1)  # repeat indefinitely\n",
    "    .batch(batch_size, drop_remainder=True)\n",
    "    .prefetch(-1))\n",
    "valid_dataset = tfds.as_numpy(\n",
    "    tf.data.Dataset.from_tensor_slices(valid_data_dict)\n",
    "    .map(cast_and_normalise_images)\n",
    "    .repeat(1)  # 1 epoch\n",
    "    .batch(batch_size)\n",
    "    .prefetch(-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "devices = jax.devices()\n",
    "# Incorrect usage: trying to call devices as if it were a function\n",
    "some_device = devices[0]  # This will cause an error if devices is not c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_forward = HaikuModule(Encoder, num_hiddens, num_residual_layers, num_residual_hiddens, name=\"encoder\")\n",
    "decoder_forward = HaikuModule(Decoder, num_hiddens, num_residual_layers, num_residual_hiddens, name=\"decoder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def f(obj, *args, **kwargs):\n",
    "#       # When the HaikuModule instance is called with input data (*args, **kwargs), it first retrieves or initializes the model's state using self.get('state', self.transformed.init, rng(), *args, **kwargs). \n",
    "#       # This step involves either fetching the existing model parameters from the module's internal state or initializing them if they haven't been created yet.\n",
    "      \n",
    "#       # The first time the HaikuModule instance is called with input data, it proceeds as follows:\n",
    "#       # 1. It attempts to retrieve the model's state (parameters) using a method like self.get. If the parameters do not yet exist in the module's context (state management system), it calls self.transformed.init to initialize them. This is where init is indirectly invoked for parameter initialization.\n",
    "#       # 2. After initialization, the parameters are stored in the module's context, making them retrievable for subsequent calls.\n",
    "#       # print(f\"args: {args}\")\n",
    "#     print(f\"args len: {len(args)}\")\n",
    "      \n",
    "#     print(f\"kwargs: {kwargs}\")\n",
    "    \n",
    "#     print(type(obj.get))\n",
    "    \n",
    "#     rng_key = jax.random.PRNGKey(42)\n",
    "    \n",
    "#     print(rng_key)\n",
    "    \n",
    "#     state = obj.get('state', obj.forward.init, rng_key, *args, **kwargs)\n",
    "#     print(f\"state: {state}\")\n",
    "#     return state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ninjax import pure\n",
    "\n",
    "\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "\n",
    "# encoder_forward(next(train_dataset_iter))\n",
    "\n",
    "\n",
    "initial_state = {}\n",
    "rng_key = jax.random.PRNGKey(42)\n",
    "\n",
    "encoder_forward_func = encoder_forward.__call__\n",
    "f_pure = pure(encoder_forward_func)\n",
    "f_pure(initial_state, rng_key, next(train_dataset_iter))\n",
    "\n",
    "# f_pure = pure(f)\n",
    "\n",
    "# f_pure(initial_state, rng_key, encoder_forward, next(train_dataset_iter))\n",
    "\n",
    "\n",
    "# params = encoder_forward(next(train_dataset_iter), is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(params, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(data, is_training):\n",
    "  encoder = HaikuModule(Encoder, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "  decoder = HaikuModule(Decoder, num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "  pre_vq_conv1 = HaikuModule(\n",
    "    hk.Conv2D,\n",
    "        output_channels=embedding_dim,\n",
    "        kernel_shape=(1, 1),\n",
    "        stride=(1, 1),\n",
    "        name=\"to_vq\"\n",
    "        )\n",
    "  if vq_use_ema:\n",
    "      vq_vae = HaikuModule(\n",
    "        hk.nets.VectorQuantizerEMA,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_embeddings=num_embeddings,\n",
    "        commitment_cost=commitment_cost,\n",
    "        decay=decay\n",
    "        )\n",
    "  else:\n",
    "      vq_vae = HaikuModule(\n",
    "        hk.nets.VectorQuantizer,\n",
    "        embedding_dim=embedding_dim,\n",
    "        num_embeddings=num_embeddings,\n",
    "        commitment_cost=commitment_cost,\n",
    "        decay=decay) \n",
    "        \n",
    "  model = HaikuModule(\n",
    "          ImageEncoderVQVAE,\n",
    "          encoder, \n",
    "          vq_vae, \n",
    "          pre_vq_conv1,\n",
    "          data_variance=train_data_variance\n",
    "          ) \n",
    "\n",
    "  return model(data['image'], is_training)\n",
    "\n",
    "forward = hk.transform_with_state(forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_iter = iter(train_dataset)\n",
    "params, state = forward.init(rng, next(train_dataset_iter), is_training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_variance = 1 # Useless\n",
    "encoder = Encoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "decoder = Decoder(num_hiddens, num_residual_layers, num_residual_hiddens)\n",
    "\n",
    "    \n",
    "pre_vq_conv1 = hk.Conv2D(\n",
    "        output_channels=embedding_dim,\n",
    "        kernel_shape=(1, 1),\n",
    "        stride=(1, 1),\n",
    "        name=\"to_vq\")\n",
    "    \n",
    "if vq_use_ema:\n",
    "    vq_vae = hk.nets.VectorQuantizerEMA(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_embeddings=num_embeddings,\n",
    "            commitment_cost=commitment_cost,\n",
    "            decay=decay)\n",
    "else:\n",
    "    vq_vae = hk.nets.VectorQuantizer(\n",
    "            embedding_dim=embedding_dim,\n",
    "            num_embeddings=num_embeddings,\n",
    "            commitment_cost=commitment_cost)   \n",
    "       \n",
    "# model = HaikuModule(\n",
    "#         ImageEncoderVQVAE,\n",
    "#         encoder, \n",
    "#         vq_vae, \n",
    "#         pre_vq_conv1,\n",
    "#         data_variance=train_data_variance\n",
    "#         ) \n",
    "# optimizer = OptaxModule(optax.adam, learning_rate) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(opt_state, data):\n",
    "    # Example function to calculate loss and update model parameters\n",
    "    def loss_fn(params, state, inputs):\n",
    "        outputs = model(inputs)  # Directly call the model\n",
    "        loss = jnp.mean((outputs - inputs) ** 2)  # Example loss calculation\n",
    "        return loss, outputs\n",
    "\n",
    "    # Use JAX to calculate the gradients, and update the parameters\n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux=True)\n",
    "    (loss, outputs), grads = grad_fn(params, state, data['image'])\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss, outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "train_losses = []\n",
    "train_recon_errors = []\n",
    "train_perplexities = []\n",
    "train_vqvae_loss = []\n",
    "\n",
    "rng = jax.random.PRNGKey(42)\n",
    "train_dataset_iter = iter(train_dataset)\n",
    "params, state = forward.init(rng, next(train_dataset_iter), is_training=True)\n",
    "opt_state = optimizer.init(params)\n",
    "\n",
    "for step in range(1, num_training_updates + 1):\n",
    "  data = next(train_dataset_iter)\n",
    "  params, state, opt_state, train_results = (\n",
    "      train_step(params, state, opt_state, data))\n",
    "\n",
    "  train_results = jax.device_get(train_results)\n",
    "  train_losses.append(train_results['loss'])\n",
    "  train_recon_errors.append(train_results['recon_error'])\n",
    "  train_perplexities.append(train_results['vq_output']['perplexity'])\n",
    "  train_vqvae_loss.append(train_results['vq_output']['loss'])\n",
    "\n",
    "  if step % 100 == 0:\n",
    "    print(f'[Step {step}/{num_training_updates}] ' + \n",
    "          ('train loss: %f ' % np.mean(train_losses[-100:])) +\n",
    "          ('recon_error: %.9f ' % np.mean(train_recon_errors[-100:])) +\n",
    "          ('perplexity: %.9f ' % np.mean(train_perplexities[-100:])) +\n",
    "          ('vqvae loss: %.9f' % np.mean(train_vqvae_loss[-100:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = '/home/lyk/Projects/Machine-Learning-Basic/VAE/checkpoints/VQ-VAE-DeepMindâ€”BouncingBall.pkl'\n",
    "\n",
    "with open(checkpoint_path, 'rb') as file:\n",
    "    loaded_model_dict = pickle.load(file)\n",
    "\n",
    "# Extract the params and state\n",
    "loaded_params = loaded_model_dict['params']\n",
    "loaded_state = loaded_model_dict['state']\n",
    "\n",
    "params = loaded_params\n",
    "state = loaded_state\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstructions\n",
    "train_batch = next(iter(train_dataset))\n",
    "valid_batch = next(iter(valid_dataset))\n",
    "\n",
    "# Put data through the model with is_training=False, so that in the case of \n",
    "# using EMA the codebook is not updated.\n",
    "train_reconstructions = forward.apply(params, state, rng, train_batch, is_training=False)[0]['x_recon']\n",
    "valid_reconstructions = forward.apply(params, state, rng, valid_batch, is_training=False)[0]['x_recon']\n",
    "\n",
    "\n",
    "\n",
    "def convert_batch_to_image_grid(image_batch, rows=4, cols=8):\n",
    "    # Assuming image_batch is of shape (B, H, W, C)\n",
    "    B, H, W, C = image_batch.shape\n",
    "    assert B >= rows * cols, \"Not enough images to fill the grid\"\n",
    "    \n",
    "    reshaped = image_batch[:rows * cols].reshape(rows, cols, H, W, C)\n",
    "    reshaped = reshaped.transpose(0, 2, 1, 3, 4)  # Transpose to (rows, H, cols, W, C)\n",
    "    grid = reshaped.reshape(rows * H, cols * W, C)\n",
    "    \n",
    "    return grid\n",
    "\n",
    "# Assuming 'train_batch', 'train_reconstructions', 'valid_batch', and 'valid_reconstructions' are available\n",
    "f = plt.figure(figsize=(16, 16))\n",
    "\n",
    "# Training Data Originals\n",
    "ax = f.add_subplot(2, 2, 1)\n",
    "ax.imshow(convert_batch_to_image_grid(train_batch['image']), interpolation='nearest')\n",
    "ax.set_title('Training Data Originals')\n",
    "plt.axis('off')\n",
    "\n",
    "# Training Data Reconstructions\n",
    "ax = f.add_subplot(2, 2, 2)\n",
    "ax.imshow(convert_batch_to_image_grid(train_reconstructions), interpolation='nearest')\n",
    "ax.set_title('Training Data Reconstructions')\n",
    "plt.axis('off')\n",
    "\n",
    "# Validation Data Originals\n",
    "ax = f.add_subplot(2, 2, 3)\n",
    "ax.imshow(convert_batch_to_image_grid(valid_batch['image']), interpolation='nearest')\n",
    "ax.set_title('Validation Data Originals')\n",
    "plt.axis('off')\n",
    "\n",
    "# Validation Data Reconstructions\n",
    "ax = f.add_subplot(2, 2, 4)\n",
    "ax.imshow(convert_batch_to_image_grid(valid_reconstructions), interpolation='nearest')\n",
    "ax.set_title('Validation Data Reconstructions')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DreamerV3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
